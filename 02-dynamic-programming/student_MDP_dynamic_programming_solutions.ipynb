{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic programming solutions to student toy example: policy evaluation and value iteration\n",
    "Implementation of policy evaluation and value iteration algorithms are copied directly from [Denny Britz](https://github.com/dennybritz/reinforcement-learning/blob/master/DP), with very minor modifications for compatibility with the `StudentEnv` class.  The student example itself is taken from David Silver's lecture 2 [notes](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf) on RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "if \"./\" not in sys.path:\n",
    "    sys.path.append(\"./\")\n",
    "from discrete_limit_env import StudentEnv\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The student MDP:\n",
    "![image](student_mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map integers used in the environment for states/actions to names for pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from integer to state names\n",
    "obs = {0: 'FACEBOOK', 1: 'CLASS1', 2: 'CLASS2', 3: 'CLASS3', 4: 'SLEEP'}\n",
    "\n",
    "# allowed actions per state \n",
    "actions_for_obs = {\n",
    "    0: {0: 'facebook', 1: 'quit'},\n",
    "    1: {0: 'facebook', 1: 'study'},\n",
    "    2: {0: 'sleep', 1: 'study'},\n",
    "    3: {0: 'pub', 1: 'study'},\n",
    "    4: {0: 'sleep'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the student environment\n",
    "env = StudentEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy evaluation\n",
    "Evaluate the value function for a given policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0.5, 0.5]), 1: array([0.5, 0.5]), 2: array([0.5, 0.5]), 3: array([0.5, 0.5]), 4: array([1.])}\n"
     ]
    }
   ],
   "source": [
    "# Random policy: agent chooses action randomly in each state\n",
    "random_policy = dict()\n",
    "for s, actions in actions_for_obs.items():\n",
    "    n_actions = len(actions)\n",
    "    random_policy[s] = np.ones(n_actions) / n_actions\n",
    "    # equivalent to pi(a|s)=0.5 for all states except the terminal sleep state\n",
    "    \n",
    "print(random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copied from https://github.com/dennybritz/reinforcement-learning/blob/master/DP/Value%20Iteration%20Solution.ipynb\n",
    "# note: no modifications needed for use with the DiscreteLimitActionsEnv.\n",
    "\n",
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.vA is a vector of the number of actions per state in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate the expected value. Ref: Sutton book eq. 4.6.\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_fxn = policy_eval(random_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal state-value in state FACEBOOK:  -2.3\n",
      "optimal state-value in state CLASS1:  -1.3\n",
      "optimal state-value in state CLASS2:  2.7\n",
      "optimal state-value in state CLASS3:  7.4\n",
      "optimal state-value in state SLEEP:  0.0\n"
     ]
    }
   ],
   "source": [
    "for s, value in enumerate(value_fxn):\n",
    "    print(f\"optimal state-value in state {obs[s]}: \", round(value,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](student_mdp_values_random_policy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration\n",
    "Simultaneously solve for the optimal value function and optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://github.com/dennybritz/reinforcement-learning/blob/master/DP/Value%20Iteration%20Solution.ipynb\n",
    "# with slight alterations for compatibility with the action_space in DiscreteLimitActionsEnv.\n",
    "\n",
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.vA is a vector of the number of actions per state in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.vA[state] containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.vA[state])\n",
    "        for a in range(env.vA[state]):\n",
    "            for prob, next_state, reward, _ in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value        \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = [np.zeros(nA) for nA in env.vA]\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s][best_action] = 1.0\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy, optimal_value_fxn = value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  6.,  8., 10.,  0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_value_fxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 1.]),\n",
       " array([0., 1.]),\n",
       " array([0., 1.]),\n",
       " array([0., 1.]),\n",
       " array([1.])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In state FACEBOOK\n",
      "optimal state-value:  6.0\n",
      "action for optimal policy:  quit \n",
      "\n",
      "In state CLASS1\n",
      "optimal state-value:  6.0\n",
      "action for optimal policy:  study \n",
      "\n",
      "In state CLASS2\n",
      "optimal state-value:  8.0\n",
      "action for optimal policy:  study \n",
      "\n",
      "In state CLASS3\n",
      "optimal state-value:  10.0\n",
      "action for optimal policy:  study \n",
      "\n",
      "In state SLEEP\n",
      "optimal state-value:  0.0\n",
      "action for optimal policy:  sleep \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s, actions in enumerate(optimal_policy):\n",
    "    print(f\"In state {obs[s]}\")\n",
    "    print(f\"optimal state-value: \", optimal_value_fxn[s])\n",
    "    print(f\"action for optimal policy: \", actions_for_obs[s][np.argmax(actions)], '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](student_mdp_optimal_values.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
