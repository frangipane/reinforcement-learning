# DQN / Atari

[Human-level control through deep reinforcement
learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)

Mnih, et al. Nature 2015

## Set up virtual environment

### pre-reqs
- `ffmpeg` for recording videos using the `Monitor` OpenAI gym wrapper: `brew install ffmpeg`
- `openmpi` for Spinning Up: `brew install openmpi`

Two choices for setting up the environment: conda or pipenv.

### conda
The environment is specified in `environment.yml`.
```shell
# Create the environment
conda env create -f environment.yml

# Activate the env
conda activate dqn

# Deactivate the env
conda deactivate
```

### pipenv
The environment is specified in `Pipfile`.
```shell
# Create the environment
pipenv install

# Activate the environment
pipenv shell

# Deactivate the env
exit
```
### [Required] Additional installation

With either your pyenv or conda environment activated, follow
[instructions](https://spinningup.openai.com/en/latest/user/installation.html#installing-spinning-up)
for installing spinning up in RL (sadly not available as a package on pypi or conda).

We use Spinning up's logger utilities.


## Run

- Activate the environment
- `python dqn.py`


## Key concepts from DQN

First successful usage of deep neural network as value function
approximator in reinforcement learning.

### General idea

- Q-learning (no explicit policy since policy comes
  naturally out of the Q values (selecting action that maximizes Q
  from any state).
- Model-free: using action-values Q(s,a) instead of state-values V(s)
  allows us to figure out how to act without knowing environment
  dynamics.
- Off-policy: samples stored in replay buffer to use for
  training/updates of Q-network were generated by different
  policies/previous Qs.  Note, the ability to use off-policy samples
  is an advantage of Q-learning, i.e. sample efficiency, since
  learning is not confined only to samples generated by current
  policy.
- Learning target: in paper, corresponds to TD(0) (temporal difference
  learning)?

## Key contributions from paper

- Experience replay: use randomized samples stored in "replay buffer"
  to stabilize training (since consecutive samples in a trajectory are
  highly correlated).
  
- Freeze target Q network for multiple steps during learning process,
  also for stability reasons (otherwise, updates to neural network
  shared parameters \Theta would affect both the target and learner,
  can lead to divergence of Bellman error).

## Methods: Algorithm 1: deep-Q-learning with experience replay.

```
Initialize replay memory D to capacity N.
Initialize action-value function $Q$ with random weights $\Theta$.
Initialize target action-value function $\hat(Q)$ with weights $\Theta^- \leftarrow \Theta$.

**For** episode = 1,M, **do**
  Initialize sequence $s_1 = {x_1}$ and preprocessed sequence $\phi_1= \phi(s_1)$.
  
  **For** t=1,T **do**
    With probability `epsilon` select a random action $a_t$
      otherwise select $a_t = \text{arg} \max_a Q(\phi(s_t), a; \Theta)$.
    Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$.
    Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$.
    Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in D.
    Sample random minibatch of transitions $(\phi_j, a_t, r_t, \phi_{j+1})$ from D.
    Set 
    $$
    y_j = \left\{
           \\begin{array}{ll}
           r_j, \text{if episode terminates at step j+1}\\
           r_j + \gamma \max_{a'} \hat(Q)(\phi_{j+1}, a'; \Theta^-), \text{otherwise}
           \\end{array}
    }
    $$
    Perform a gradient descent step on $(y_j - Q(\phi_j,a_j;\Theta))^2$
      w.r.t. network parameters $\Theta$.
    Every C steps reset $\hat(Q) = Q$
  **End For**
**End For**
```

## Extended Data Table 1: Hyperparameters

![DQN hyperparams](dqn_hyperparams.png)
